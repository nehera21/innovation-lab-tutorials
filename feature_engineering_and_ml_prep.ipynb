{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d566eef",
   "metadata": {},
   "source": [
    "# Tutorial 2: Feature Engineering and ML Preparation\n",
    "\n",
    "Welcome back! In the previous tutorial, we learned how to load and clean data. Now we'll take the next step: **transforming our clean data into features that machine learning models can learn from effectively**.\n",
    "\n",
    "## What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the art and science of creating new features from existing data to improve model performance. It's often said that:\n",
    "\n",
    "> *\"Better features beat better algorithms.\"*\n",
    "\n",
    "A simple model with great features often outperforms a complex model with poor features!\n",
    "\n",
    "## Why Does This Matter for ML?\n",
    "\n",
    "- **Better Predictions**: Well-engineered features help models find patterns more easily\n",
    "- **Reduced Complexity**: Good features can make simple models work as well as complex ones\n",
    "- **Domain Knowledge**: Feature engineering lets you incorporate what you know about the problem\n",
    "- **Model Performance**: Can dramatically improve accuracy, precision, and other metrics\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. **Feature Scaling & Normalization** - Making features comparable\n",
    "2. **Encoding Techniques** - Advanced categorical encoding\n",
    "3. **Feature Creation** - Building new features from existing ones\n",
    "4. **Feature Selection** - Choosing the most important features\n",
    "5. **Train-Test Splitting** - Properly dividing data for ML\n",
    "6. **Handling Imbalanced Data** - Dealing with unequal class distributions\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea424a9",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data\n",
    "\n",
    "We'll start by importing our essential libraries and loading the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62715443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a9dfc",
   "metadata": {},
   "source": [
    "### Load and Prepare the Titanic Dataset\n",
    "\n",
    "Let's load the data and perform the basic cleaning steps we learned in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Fill missing values (using recommended pandas approach)\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "df = df.drop(columns=['deck', 'embark_town'])\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dec86",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization\n",
    "\n",
    "### Why Scale Features?\n",
    "\n",
    "Different features often have different scales:\n",
    "- **Age**: 0-80 years\n",
    "- **Fare**: 0-500+ dollars\n",
    "- **Pclass**: 1, 2, or 3\n",
    "\n",
    "Many ML algorithms (like SVM, KNN, Neural Networks) are sensitive to feature scales. Features with larger ranges can dominate the learning process!\n",
    "\n",
    "### Two Main Approaches:\n",
    "\n",
    "1. **Standardization (Z-score normalization)**: Centers data around mean=0, std=1\n",
    "   - Formula: `(x - mean) / std`\n",
    "   - Use when: Data follows normal distribution, or algorithm assumes this (SVM, Logistic Regression)\n",
    "\n",
    "2. **Min-Max Scaling**: Scales data to a fixed range [0, 1]\n",
    "   - Formula: `(x - min) / (max - min)`\n",
    "   - Use when: You need bounded values, or data doesn't follow normal distribution\n",
    "\n",
    "Let's see both in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features to scale\n",
    "numerical_features = ['age', 'fare', 'sibsp', 'parch']\n",
    "\n",
    "# Before scaling - look at the distributions\n",
    "print(\"BEFORE SCALING:\")\n",
    "print(df[numerical_features].describe())\n",
    "print(\"\\nNotice the different scales: age (0-80), fare (0-512), etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9309b7d",
   "metadata": {},
   "source": [
    "### StandardScaler (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for standardization\n",
    "df_standardized = df.copy()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_standardized[numerical_features] = scaler_standard.fit_transform(df[numerical_features])\n",
    "\n",
    "print(\"AFTER STANDARDIZATION (StandardScaler):\")\n",
    "print(df_standardized[numerical_features].describe())\n",
    "print(\"\\nNotice: mean ≈ 0, std ≈ 1 for all features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ef9a9",
   "metadata": {},
   "source": [
    "### MinMaxScaler (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for min-max scaling\n",
    "df_normalized = df.copy()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_normalized[numerical_features] = scaler_minmax.fit_transform(df[numerical_features])\n",
    "\n",
    "print(\"AFTER MIN-MAX SCALING:\")\n",
    "print(df_normalized[numerical_features].describe())\n",
    "print(\"\\nNotice: all values are between 0 and 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af3f8b",
   "metadata": {},
   "source": [
    "### Visualizing the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf343907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare age distribution across different scaling methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "df['age'].hist(bins=30, ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Original Age Distribution')\n",
    "axes[0].set_xlabel('Age (years)')\n",
    "\n",
    "# Standardized\n",
    "df_standardized['age'].hist(bins=30, ax=axes[1], color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Standardized Age (StandardScaler)')\n",
    "axes[1].set_xlabel('Age (z-score)')\n",
    "\n",
    "# Normalized\n",
    "df_normalized['age'].hist(bins=30, ax=axes[2], color='coral', edgecolor='black')\n",
    "axes[2].set_title('Normalized Age (MinMaxScaler)')\n",
    "axes[2].set_xlabel('Age (0-1 scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Shape stays the same, but the scale changes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc4430",
   "metadata": {},
   "source": [
    "## Categorical Encoding\n",
    "\n",
    "In the previous tutorial, we used simple encoding. Now let's explore more sophisticated techniques!\n",
    "\n",
    "### Encoding Methods:\n",
    "1. **Label Encoding**: Convert to integers (0, 1, 2, ...)\n",
    "2. **One-Hot Encoding**: Create binary columns\n",
    "3. **Frequency Encoding**: Replace with category frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc758c2",
   "metadata": {},
   "source": [
    "### Label Encoding (for binary or ordinal features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our standardized dataframe\n",
    "df_encoded = df_standardized.copy()\n",
    "\n",
    "# Label encode 'sex' (binary: male/female)\n",
    "label_encoder = LabelEncoder()\n",
    "df_encoded['sex_encoded'] = label_encoder.fit_transform(df_encoded['sex'])\n",
    "\n",
    "print(\"Sex encoding:\")\n",
    "print(df_encoded[['sex', 'sex_encoded']].drop_duplicates().sort_values('sex_encoded'))\n",
    "print(f\"\\nMapping: female={label_encoder.transform(['female'])[0]}, male={label_encoder.transform(['male'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19391f6e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding (for nominal features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'embarked' (nominal: no natural order)\n",
    "embarked_dummies = pd.get_dummies(df_encoded['embarked'], prefix='embarked', drop_first=True)\n",
    "\n",
    "# Add to dataframe\n",
    "df_encoded = pd.concat([df_encoded, embarked_dummies], axis=1)\n",
    "\n",
    "print(\"One-hot encoding for 'embarked':\")\n",
    "print(df_encoded[['embarked', 'embarked_Q', 'embarked_S']].head(10))\n",
    "print(\"\\nNote: We dropped first category 'embarked_C' to avoid multicollinearity which is when two or more predictor variables are highly correlated.\")\n",
    "print(\"If embarked_Q=0 and embarked_S=0, then embarked='C'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e3faa",
   "metadata": {},
   "source": [
    "### Frequency Encoding (useful for high-cardinality features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encode 'class' - how often does each class appear?\n",
    "class_freq = df_encoded['class'].value_counts(normalize=True)\n",
    "df_encoded['class_frequency'] = df_encoded['class'].map(class_freq)\n",
    "\n",
    "print(\"Frequency encoding for 'class':\")\n",
    "print(df_encoded[['class', 'class_frequency']].drop_duplicates().sort_values('class'))\n",
    "print(\"\\nThis tells us: Third class is most common (~55%), First is least (~24%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b9330",
   "metadata": {},
   "source": [
    "## Feature Engineering Revisited\n",
    "\n",
    "Now let's create some new features from existing ones! This is where domain knowledge really shines.\n",
    "\n",
    "### Domain Knowledge Applied:\n",
    "- **Families** might have different survival rates\n",
    "- **Age groups** (children, adults, elderly) had different priorities\n",
    "- **Wealth indicators** like fare per person can be more informative than total fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Family Size Features\n",
    "# Combine siblings/spouses and parents/children, then add 1 for the person themselves\n",
    "df_encoded['family_size'] = df_encoded['sibsp'] + df_encoded['parch'] + 1\n",
    "\n",
    "# Is traveling alone?\n",
    "df_encoded['is_alone'] = (df_encoded['family_size'] == 1).astype(int)\n",
    "\n",
    "print(\"Family-based features created:\")\n",
    "print(df_encoded[['sibsp', 'parch', 'family_size', 'is_alone']].head(10))\n",
    "print(f\"\\nSurvival rate - Alone: {df_encoded[df_encoded['is_alone']==1]['survived'].mean():.2%}\")\n",
    "print(f\"Survival rate - With family: {df_encoded[df_encoded['is_alone']==0]['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Age-based Features\n",
    "# Is child? (under 18 - \"children first\" policy)\n",
    "df_encoded['is_child'] = (df_encoded['age'] < 18).astype(int)\n",
    "\n",
    "# Is elderly? (over 60)\n",
    "df_encoded['is_elderly'] = (df_encoded['age'] > 60).astype(int)\n",
    "\n",
    "print(\"Age-based features created:\")\n",
    "print(df_encoded[['age', 'is_child', 'is_elderly']].head(15))\n",
    "print(f\"\\nSurvival rate - Children: {df_encoded[df_encoded['is_child']==1]['survived'].mean():.2%}\")\n",
    "print(f\"Survival rate - Adults: {df_encoded[(df_encoded['is_child']==0) & (df_encoded['is_elderly']==0)]['survived'].mean():.2%}\")\n",
    "print(f\"Survival rate - Elderly: {df_encoded[df_encoded['is_elderly']==1]['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fare per Person (Wealth Indicator)\n",
    "# Total fare might be for a whole family, so divide by family size\n",
    "df_encoded['fare_per_person'] = df_encoded['fare'] / df_encoded['family_size']\n",
    "\n",
    "print(\"Fare-based features created:\")\n",
    "print(df_encoded[['fare', 'family_size', 'fare_per_person', 'pclass']].head(10))\n",
    "\n",
    "# Compare fare per person by class\n",
    "print(\"\\nAverage fare per person by class:\")\n",
    "print(df_encoded.groupby('pclass')['fare_per_person'].mean())\n",
    "\n",
    "print(\"\\n  All new features created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27456c2d",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We've created many features! But more features ≠ better model. We need to select the most important ones.\n",
    "\n",
    "### Why Feature Selection?\n",
    "- **Reduces overfitting**: Fewer features = simpler, more generalizable model\n",
    "- **Faster training**: Less data to process\n",
    "- **Better interpretability**: Easier to understand what drives predictions\n",
    "- **Removes noise**: Irrelevant features can hurt performance\n",
    "\n",
    "### Methods:\n",
    "1. **Statistical Tests**: F-score, chi-squared\n",
    "2. **Correlation Analysis**: Which features correlate with target?\n",
    "3. **Feature Importance**: From tree-based models (bonus thing for you to explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45705f33",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c167b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical features for correlation\n",
    "numerical_cols = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate correlation with target (survived)\n",
    "correlations = df_encoded[numerical_cols].corr()['survived'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top features correlated with survival:\")\n",
    "print(correlations.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a060a57",
   "metadata": {},
   "source": [
    "### Visualize Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d94d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlated features\n",
    "top_features = correlations.head(10).index.tolist()\n",
    "top_features.remove('survived')  # Remove target itself\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_encoded[top_features + ['survived']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, linewidths=1, yticklabels=True)\n",
    "plt.title('Correlation Heatmap: Top Features vs Survival')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425ce49",
   "metadata": {},
   "source": [
    "### SelectKBest (Statistical Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded[numerical_cols].drop('survived', axis=1)\n",
    "y = df_encoded['survived']\n",
    "\n",
    "# Remove any remaining NaN values\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Select top 10 features using F-score\n",
    "selector = SelectKBest(f_classif, k=6)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(\"Top features selected by F-score:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = selector.scores_[X.columns.get_loc(feature)]\n",
    "    print(f\"{i}. {feature}: {score:.2f}\")\n",
    "\n",
    "# Visualize the scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_scores = [selector.scores_[X.columns.get_loc(feature)] for feature in selected_features]\n",
    "plt.barh(selected_features, feature_scores, color='lightblue', edgecolor='navy')\n",
    "plt.xlabel('F-score')\n",
    "plt.title('Top 6 Features Selected by F-score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d952cad",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "### Critical Concept: Never Test on Training Data!\n",
    "\n",
    "If we train and test on the same data, our model will appear perfect but fail on new data. This is called **overfitting**.\n",
    "\n",
    "### The Solution: Train-Test Split\n",
    "- **Training Set** (typically 70-80%): Used to train the model\n",
    "- **Test Set** (typically 20-30%): Used to evaluate the model\n",
    "\n",
    "The model NEVER sees the test set during training!\n",
    "\n",
    "### Important: When to Split?\n",
    "-  Split AFTER cleaning\n",
    "-  Split BEFORE feature scaling (to avoid data leakage)\n",
    "-  Use stratification for imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our final features (using top correlated features)\n",
    "final_features = ['sex_encoded', 'pclass', 'fare', 'age', 'family_size', \n",
    "                  'fare_per_person', 'is_child', 'is_alone', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "X_final = df_encoded[final_features].fillna(0)\n",
    "y_final = df_encoded['survived']\n",
    "\n",
    "# Perform the split\n",
    "# stratify=y ensures same proportion of survived/died in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y_final    # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"Dataset split completed!\")\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print(\"\\n Distributions are similar - stratification worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbeb80c",
   "metadata": {},
   "source": [
    "### Scaling AFTER Split (Critical!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit ONLY on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both sets using the same scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier viewing\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Scaling completed!\")\n",
    "print(\"\\nTraining set statistics (after scaling):\")\n",
    "print(X_train_scaled.describe())\n",
    "\n",
    "print(\"\\\"Why fit only on training data?\")\n",
    "print(\"To prevent DATA LEAKAGE: Test data information shouldn't influence training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae32c71",
   "metadata": {},
   "source": [
    "## Step 6: Handling Imbalanced Data\n",
    "\n",
    "Let's check if our classes are balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd279ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "class_distribution = y_train.value_counts()\n",
    "class_percentages = y_train.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Died (0): {class_distribution[0]} ({class_percentages[0]:.1f}%)\")\n",
    "print(f\"Survived (1): {class_distribution[1]} ({class_percentages[1]:.1f}%)\")\n",
    "\n",
    "imbalance_ratio = class_distribution[0] / class_distribution[1]\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"\\nClasses are somewhat imbalanced!\")\n",
    "    print(\"This is moderate imbalance. Consider:\")\n",
    "    print(\"- Using class_weight='balanced' in models\")\n",
    "    print(\"- Focusing on precision/recall instead of just accuracy\")\n",
    "    print(\"- Using stratified sampling (already done!)\")\n",
    "else:\n",
    "    print(\"\\nClasses are reasonably balanced!\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Died', 'Survived'], class_distribution.values, color=['coral', 'skyblue'])\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(class_distribution.values):\n",
    "    plt.text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b706c24",
   "metadata": {},
   "source": [
    "## Step 7: Final Data Summary\n",
    "\n",
    "Let's review what we've prepared for machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL ML-READY DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(f\"   Training: {X_train_scaled.shape}\")\n",
    "print(f\"   Testing:  {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(f\"   Name: 'survived' (binary: 0=died, 1=survived)\")\n",
    "print(f\"   Training samples: {len(y_train)}\")\n",
    "print(f\"   Test samples: {len(y_test)}\")\n",
    "\n",
    "print(\"\\nFeatures Used:\")\n",
    "for i, feature in enumerate(final_features, 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "\n",
    "print(\"\\nPreprocessing Applied:\")\n",
    "print(\"   - Missing values handled (median/mode imputation)\")\n",
    "print(\"   - Features scaled (StandardScaler)\")\n",
    "print(\"   - Categorical variables encoded (Label + One-Hot)\")\n",
    "print(\"   - New features engineered (family_size, fare_per_person, etc.)\")\n",
    "print(\"   - Train-test split performed (80-20)\")\n",
    "print(\"   - Stratified sampling to maintain class balance\")\n",
    "\n",
    "print(\"\\nData is ready for machine learning models!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa1799",
   "metadata": {},
   "source": [
    "### Additional Techniques to Explore:\n",
    "\n",
    "- **PCA** (Principal Component Analysis) - Dimensionality reduction\n",
    "- **Polynomial Features** - Create interaction terms automatically\n",
    "- **Target Encoding** - Use target statistics for encoding\n",
    "- **SMOTE** - Synthetic data generation for imbalanced classes\n",
    "- **Feature Importance** - From tree-based models\n",
    "\n",
    "##### Never stop learning! If you stop learning, you'll get left behind in this industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ba0a6",
   "metadata": {},
   "source": [
    "## Practice Exercise: Apply to Iris Dataset\n",
    "\n",
    "Now it's your turn! Apply these techniques to the Iris dataset from the previous tutorial.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset from `data/Iris.csv`\n",
    "2. Encode the Species column\n",
    "3. Split into train-test sets (80-20)\n",
    "4. Scale the features using StandardScaler\n",
    "5. Check if the classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c139cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "\n",
    "# Separate features and target\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# Encode species using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = ...\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Print the 4 dataset shapes\n",
    "\n",
    "# Scale features using StandardScaler (fit_transform for training vs transform for test)\n",
    "scaler = StandardScaler()\n",
    "X_train = ...\n",
    "X_test = ...\n",
    "\n",
    "# Show the mean and std of scaled features\n",
    "print(\"\\nScaled feature statistics (training set):\")\n",
    "print(f\"Mean (should be ~0): {np.mean(X_train, axis=0)}\")\n",
    "print(f\"Std (should be ~1): {np.std(X_train, axis=0)}\\n\")\n",
    "\n",
    "# Visualize class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(le.inverse_transform(unique), counts))\n",
    "classes = class_distribution.keys()\n",
    "counts = class_distribution.values()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "# Matplotlib bar chart goes here - make each bar a different color :)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336382b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
