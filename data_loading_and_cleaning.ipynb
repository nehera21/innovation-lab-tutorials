{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dba962",
   "metadata": {},
   "source": [
    "# Tutorial 1: Data Loading and Cleaning with Pandas\n",
    "\n",
    "Welcome to this comprehensive tutorial on data loading and cleaning! In this notebook, we'll explore how to work with real-world data using **pandas**, Python's most popular data manipulation library.\n",
    "\n",
    "## Why Data Cleaning Matters in Machine Learning\n",
    "\n",
    "Before we can train any machine learning model, we need to prepare our data. In fact, data scientists often spend 60-80% of their time on data preparation! Here's why:\n",
    "\n",
    "- **Real-world data is messy**: Missing values, inconsistent formats, outliers\n",
    "- **Models need clean input**: Most ML algorithms can't handle missing values or non-numeric data\n",
    "- **Quality in = Quality out**: Better data preparation leads to better model performance\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. Loading data with pandas\n",
    "2. Exploring and understanding our dataset\n",
    "3. Handling missing values\n",
    "4. Data type conversions\n",
    "5. Creating new features (feature engineering basics)\n",
    "6. Preparing data for ML models\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd62a8",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries\n",
    "\n",
    "First, we need to import the tools we'll use:\n",
    "- **pandas**: Our main data manipulation library (commonly abbreviated as `pd`)\n",
    "- **seaborn**: A visualization library that also includes sample datasets\n",
    "\n",
    "Think of importing libraries like getting tools out of a toolbox before starting a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np  # Let's also import numpy for some numerical operations\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4271a8",
   "metadata": {},
   "source": [
    "## Step 2: Loading Data\n",
    "\n",
    "Now let's load our dataset. We'll use the famous **Titanic dataset**, which contains information about passengers on the Titanic.\n",
    "\n",
    "### Why the Titanic Dataset?\n",
    "- It's a classic ML dataset for binary classification (survived or not)\n",
    "- It has realistic data quality issues (missing values, mixed data types)\n",
    "- It's relatively small and easy to understand\n",
    "\n",
    "### What is a DataFrame?\n",
    "When we load data with pandas, it creates a **DataFrame** - think of it as a smart spreadsheet:\n",
    "- **Rows**: Individual records (each passenger)\n",
    "- **Columns**: Features/attributes (age, fare, cabin, etc.)\n",
    "- **Index**: Row labels (by default, numbered 0, 1, 2, ...)\n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaa779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset from seaborn\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Display a success message\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape of the dataset: {df.shape}\")  # (rows, columns)\n",
    "print(f\"We have {df.shape[0]} passengers and {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e4bb5",
   "metadata": {},
   "source": [
    "## Step 3: Initial Data Exploration\n",
    "\n",
    "Before cleaning data, we need to **understand** what we're working with. Let's use some essential pandas functions to explore our dataset.\n",
    "\n",
    "### The `.head()` Method\n",
    "This shows the first few rows of our DataFrame (default is 5 rows). It's like peeking at the top of a spreadsheet to see what kind of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925eca4a",
   "metadata": {},
   "source": [
    "### The `.info()` Method\n",
    "\n",
    "This is one of the **most important** methods for data exploration! It shows:\n",
    "- **Column names**: What features we have\n",
    "- **Data types**: Whether each column is numeric, text (object), etc.\n",
    "- **Non-null count**: How many non-missing values in each column\n",
    "- **Memory usage**: How much space the data takes\n",
    "\n",
    "This is crucial for ML because it immediately shows us:\n",
    "1. Which columns have missing values\n",
    "2. Which columns need type conversion\n",
    "3. The overall structure of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f74d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecc338",
   "metadata": {},
   "source": [
    "### The `.describe()` Method\n",
    "\n",
    "This provides **statistical summaries** for numerical columns:\n",
    "- **count**: Number of non-missing values\n",
    "- **mean**: Average value\n",
    "- **std**: Standard deviation (spread of data)\n",
    "- **min/max**: Smallest and largest values\n",
    "- **25%, 50%, 75%**: Quartiles (percentiles)\n",
    "\n",
    "This helps us spot:\n",
    "- Outliers (extreme values)\n",
    "- Scale differences between features\n",
    "- Distribution characteristics\n",
    "\n",
    "For ML, knowing these statistics helps us decide if we need to normalize or scale features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140def0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076eaa3a",
   "metadata": {},
   "source": [
    "### Understanding the Columns\n",
    "\n",
    "Let's look at what each column means:\n",
    "- **survived**: Our target variable (0 = died, 1 = survived) - what we want to predict!\n",
    "- **pclass**: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd) - socioeconomic status\n",
    "- **sex**: Gender of passenger\n",
    "- **age**: Age in years\n",
    "- **sibsp**: Number of siblings/spouses aboard\n",
    "- **parch**: Number of parents/children aboard\n",
    "- **fare**: Ticket price\n",
    "- **embarked**: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "- **class**: Text version of pclass\n",
    "- **deck**: Deck level (extracted from cabin)\n",
    "- **alive**: Text version of survived\n",
    "\n",
    "### Accessing Individual Columns\n",
    "\n",
    "In pandas, we can access a single column using bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single column - this returns a Series (1-dimensional)\n",
    "print(\"Ages of first 10 passengers:\")\n",
    "print(df['age'].head(10))\n",
    "\n",
    "# We can also use .value_counts() to see the distribution of categorical values\n",
    "print(\"\\n\\nSurvival distribution:\")\n",
    "print(df['survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fdf2c",
   "metadata": {},
   "source": [
    "## Step 4: Handling Missing Values\n",
    "\n",
    "Missing values are **the most common data quality issue** in real-world datasets. Most ML algorithms cannot handle missing values, so we must deal with them!\n",
    "\n",
    "### Detecting Missing Values\n",
    "\n",
    "Pandas uses `NaN` (Not a Number) to represent missing values. Let's use `.isnull()` and `.sum()` to count missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Show only columns with missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "print(\"\\n\\nPercentage of missing values:\")\n",
    "missing_percentage = (missing_values[missing_values > 0] / len(df)) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba945529",
   "metadata": {},
   "source": [
    "### Strategies for Handling Missing Values\n",
    "\n",
    "We have several options when dealing with missing values:\n",
    "\n",
    "1. **Drop rows/columns**: If very few values are missing or the column isn't important\n",
    "2. **Fill with a constant**: Use a placeholder value (e.g., 0, \"Unknown\")\n",
    "3. **Fill with statistics**: Use mean, median, or mode\n",
    "4. **Forward/backward fill**: Use previous/next value (for time series)\n",
    "5. **Predictive imputation**: Use ML to predict missing values (advanced)\n",
    "\n",
    "### Strategy for Our Dataset\n",
    "\n",
    "- **age**: ~20% missing - we'll fill with the **median** (robust to outliers)\n",
    "- **embarked**: Only 2 missing - we'll fill with the **mode** (most common value)\n",
    "- **deck**: ~77% missing - we'll **drop** this column (too much missing data)\n",
    "- **embark_town**: Similar to embarked, we'll drop it to avoid redundancy\n",
    "\n",
    "Let's implement these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe to preserve the original\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Fill missing age values with median\n",
    "# Why median? It's less affected by outliers than mean\n",
    "median_age = df_clean['age'].median()\n",
    "df_clean['age'].fillna(median_age, inplace=True)\n",
    "print(f\"Filled {df['age'].isnull().sum()} missing age values with median: {median_age}\")\n",
    "\n",
    "# 2. Fill missing embarked values with mode (most common value)\n",
    "mode_embarked = df_clean['embarked'].mode()[0]  # mode() returns a Series, we take first value\n",
    "df_clean['embarked'].fillna(mode_embarked, inplace=True)\n",
    "print(f\"Filled {df['embarked'].isnull().sum()} missing embarked values with mode: {mode_embarked}\")\n",
    "\n",
    "# 3. Drop columns with too many missing values\n",
    "columns_to_drop = ['deck', 'embark_town']\n",
    "df_clean.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"\\nDropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Verify we've handled all missing values\n",
    "print(\"\\n\\nRemaining missing values:\")\n",
    "print(df_clean.isnull().sum().sum())  # Double .sum() gives total missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3acbc03",
   "metadata": {},
   "source": [
    "## Step 5: Data Type Conversions\n",
    "\n",
    "Different data types are handled differently by ML algorithms. Let's look at what we have:\n",
    "\n",
    "### Types of Data in ML:\n",
    "1. **Numerical (Continuous)**: Age, fare - can be any value in a range\n",
    "2. **Numerical (Discrete)**: Survived, pclass - specific integer values\n",
    "3. **Categorical**: Sex, embarked - limited set of categories\n",
    "4. **Text**: Name - free-form text (often need special handling)\n",
    "\n",
    "Let's check our current data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1079d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types in our cleaned dataset:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# Let's also see unique values for categorical columns\n",
    "print(\"\\n\\nUnique values in 'sex':\", df_clean['sex'].unique())\n",
    "print(\"Unique values in 'embarked':\", df_clean['embarked'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed975b8b",
   "metadata": {},
   "source": [
    "### Converting Categorical Variables to Numbers\n",
    "\n",
    "Most ML algorithms require **numerical input**. We need to convert categorical variables like 'sex' and 'embarked' to numbers.\n",
    "\n",
    "Two main approaches:\n",
    "1. **Label Encoding**: Convert categories to numbers (0, 1, 2, ...) - good for ordinal data\n",
    "2. **One-Hot Encoding**: Create binary columns for each category - good for nominal data\n",
    "\n",
    "For 'sex' (only 2 categories), we can use simple mapping.\n",
    "For 'embarked' (3 categories with no order), we'll use one-hot encoding with `pd.get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0794fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode 'sex' as binary (male=1, female=0)\n",
    "df_clean['sex_encoded'] = df_clean['sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# 2. One-hot encode 'embarked' (creates separate columns for each value)\n",
    "# drop_first=True prevents multicollinearity (avoids redundant information)\n",
    "embarked_dummies = pd.get_dummies(df_clean['embarked'], prefix='embarked', drop_first=True)\n",
    "\n",
    "# Add the new columns to our dataframe\n",
    "df_clean = pd.concat([df_clean, embarked_dummies], axis=1)\n",
    "\n",
    "print(\"New columns created:\")\n",
    "print(df_clean[['sex', 'sex_encoded', 'embarked', 'embarked_Q', 'embarked_S']].head())\n",
    "\n",
    "print(\"\\n\\nEncoding explanation:\")\n",
    "print(\"- sex_encoded: 1 if male, 0 if female\")\n",
    "print(\"- embarked_Q: 1 if embarked at Queenstown, 0 otherwise\")\n",
    "print(\"- embarked_S: 1 if embarked at Southampton, 0 otherwise\")\n",
    "print(\"- If both Q and S are 0, passenger embarked at Cherbourg (C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ab92b",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering\n",
    "\n",
    "**Feature engineering** is creating new features from existing ones. Good features can dramatically improve ML model performance!\n",
    "\n",
    "### Creating a Family Size Feature\n",
    "\n",
    "Let's combine `sibsp` (siblings/spouses) and `parch` (parents/children) to create a `family_size` feature. Hypothesis: Larger families might have different survival rates.\n",
    "\n",
    "### Creating an Age Group Feature\n",
    "\n",
    "Instead of exact age, let's categorize passengers into age groups. This can help the model learn patterns more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create family_size feature\n",
    "# Add 1 to include the passenger themselves\n",
    "df_clean['family_size'] = df_clean['sibsp'] + df_clean['parch'] + 1\n",
    "\n",
    "# 2. Create is_alone feature (binary: traveling alone or not)\n",
    "df_clean['is_alone'] = (df_clean['family_size'] == 1).astype(int)\n",
    "\n",
    "# 3. Create age groups using pd.cut()\n",
    "# This bins continuous values into discrete categories\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'], \n",
    "                                bins=[0, 12, 18, 35, 60, 100], \n",
    "                                labels=['Child', 'Teen', 'Adult', 'Middle Age', 'Senior'])\n",
    "\n",
    "# Display some examples\n",
    "print(\"New features created:\")\n",
    "print(df_clean[['sibsp', 'parch', 'family_size', 'is_alone', 'age', 'age_group']].head(10))\n",
    "\n",
    "# Check distribution of new features\n",
    "print(\"\\n\\nAge group distribution:\")\n",
    "print(df_clean['age_group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea0aed",
   "metadata": {},
   "source": [
    "## Step 7: Selecting Features for ML\n",
    "\n",
    "Now we need to decide which columns to keep for our ML model. We want:\n",
    "- **Relevant features**: Columns that help predict survival\n",
    "- **Numerical features**: Already encoded or numeric\n",
    "- **No redundancy**: Avoid duplicate information\n",
    "\n",
    "Let's select the most useful columns and drop:\n",
    "- Text columns that haven't been encoded (name, sex, embarked, alive, class)\n",
    "- Our target variable will be kept separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66985f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML model\n",
    "feature_columns = ['pclass', 'sex_encoded', 'age', 'sibsp', 'parch', \n",
    "                   'fare', 'embarked_Q', 'embarked_S', 'family_size', 'is_alone']\n",
    "\n",
    "# Create feature matrix (X) and target vector (y)\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['survived']\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "print(\"\\n\\nFeatures we'll use for ML:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Display first few rows of our ML-ready data\n",
    "print(\"\\n\\nFirst 5 rows of feature matrix:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c84dbe0",
   "metadata": {},
   "source": [
    "## Step 8: Final Data Quality Check\n",
    "\n",
    "Before using our data for ML, let's verify everything is clean and ready:\n",
    "\n",
    "### Checklist:\n",
    "âœ“ No missing values\n",
    "âœ“ All features are numerical\n",
    "âœ“ No infinite or extreme outliers (optional, depends on algorithm)\n",
    "âœ“ Data types are appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in feature matrix:\", X.isnull().sum().sum())\n",
    "print(\"Missing values in target vector:\", y.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n\\nData types:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "# Check for any infinite values\n",
    "print(\"\\n\\nInfinite values:\", np.isinf(X).sum().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\\nFinal summary statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "print(\"\\n\\nâœ… Data is clean and ready for machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0585ad",
   "metadata": {},
   "source": [
    "## Bonus: Quick Data Visualization\n",
    "\n",
    "While not strictly part of data cleaning, visualization helps us understand our data better. Let's create a few simple visualizations using pandas built-in plotting (powered by matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Survival rate by passenger class\n",
    "df_clean.groupby('pclass')['survived'].mean().plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Survival Rate by Passenger Class')\n",
    "axes[0, 0].set_xlabel('Passenger Class')\n",
    "axes[0, 0].set_ylabel('Survival Rate')\n",
    "axes[0, 0].set_xticklabels(['1st', '2nd', '3rd'], rotation=0)\n",
    "\n",
    "# 2. Age distribution\n",
    "df_clean['age'].hist(bins=30, ax=axes[0, 1], color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Age Distribution')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Survival rate by sex\n",
    "df_clean.groupby('sex')['survived'].mean().plot(kind='bar', ax=axes[1, 0], color='coral')\n",
    "axes[1, 0].set_title('Survival Rate by Sex')\n",
    "axes[1, 0].set_xlabel('Sex')\n",
    "axes[1, 0].set_ylabel('Survival Rate')\n",
    "axes[1, 0].set_xticklabels(['Female', 'Male'], rotation=0)\n",
    "\n",
    "# 4. Family size distribution\n",
    "df_clean['family_size'].value_counts().sort_index().plot(kind='bar', ax=axes[1, 1], color='plum')\n",
    "axes[1, 1].set_title('Family Size Distribution')\n",
    "axes[1, 1].set_xlabel('Family Size')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insights from visualizations:\")\n",
    "print(\"- First class passengers had higher survival rates\")\n",
    "print(\"- Women had significantly higher survival rates (\\\"women and children first\\\")\")\n",
    "print(\"- Most passengers traveled alone or with small families\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9027aaf",
   "metadata": {},
   "source": [
    "## Summary: What We've Learned\n",
    "\n",
    "Congratulations! You've completed a full data cleaning pipeline. Let's recap the key pandas concepts and techniques:\n",
    "\n",
    "### Essential Pandas Functions We Used:\n",
    "\n",
    "1. **Data Loading & Exploration**\n",
    "   - `pd.read_csv()` / `sns.load_dataset()` - Load data\n",
    "   - `.head()`, `.tail()` - View first/last rows\n",
    "   - `.info()` - Get column info and data types\n",
    "   - `.describe()` - Statistical summaries\n",
    "   - `.shape` - Dimensions of data\n",
    "\n",
    "2. **Data Cleaning**\n",
    "   - `.isnull()`, `.sum()` - Detect missing values\n",
    "   - `.fillna()` - Fill missing values\n",
    "   - `.drop()` - Remove columns or rows\n",
    "   - `.copy()` - Create a copy of DataFrame\n",
    "\n",
    "3. **Data Transformation**\n",
    "   - `.map()` - Apply mapping to values\n",
    "   - `pd.get_dummies()` - One-hot encoding\n",
    "   - `pd.concat()` - Combine DataFrames\n",
    "   - `pd.cut()` - Bin continuous values\n",
    "   - `.astype()` - Convert data types\n",
    "\n",
    "4. **Data Analysis**\n",
    "   - `.value_counts()` - Count unique values\n",
    "   - `.groupby()` - Group and aggregate data\n",
    "   - `.unique()` - Get unique values\n",
    "   - Bracket notation `[]` - Select columns\n",
    "\n",
    "### The Data Cleaning Process:\n",
    "\n",
    "1. **Load** the data\n",
    "2. **Explore** to understand structure and issues\n",
    "3. **Handle** missing values (drop, fill, or impute)\n",
    "4. **Convert** data types and encode categories\n",
    "5. **Engineer** new features from existing ones\n",
    "6. **Select** relevant features for modeling\n",
    "7. **Verify** data quality before ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1408b",
   "metadata": {},
   "source": [
    "## Practice Exercises:\n",
    "\n",
    "Now it's your turn! Let's apply what we've learned to a different dataset - the famous **Iris flower dataset**. This dataset contains measurements of iris flowers (petal and sepal) from three different species.\n",
    "\n",
    "We used `sns.load_dataset()`, but in real projects you'll often have CSV files. To load this ino a dataframe, use `pd.read_csv()`:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('filename.csv')  # Basic usage\n",
    "df = pd.read_csv('path/to/file.csv')  # With path\n",
    "```\n",
    "\n",
    "The Iris dataset is in the `data` folder. Let's load it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb59894",
   "metadata": {},
   "source": [
    "### Exercise 1: Load and Explore the Data\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset from `data/Iris.csv` using `pd.read_csv()`\n",
    "2. Display the first 10 rows\n",
    "3. Use `.info()` to check for data types and missing values\n",
    "4. Use `.describe()` to see statistical summaries\n",
    "5. Check how many samples of each species we have (hint: use `.value_counts()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. Load the dataset\n",
    "iris_df = None  # Replace None with the correct code to load the dataset\n",
    "\n",
    "# TODO: 2. Display first 10 rows\n",
    "\n",
    "\n",
    "# TODO: 3. Check dataframe info using .info()\n",
    "\n",
    "\n",
    "# TODO: 4. Show the statistical summary\n",
    "\n",
    "\n",
    "# TODO: 5. Count of each species (hint: use .value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fafa6",
   "metadata": {},
   "source": [
    "### Exercise 2: Clean and Prepare for ML\n",
    "\n",
    "**Tasks:**\n",
    "1. Drop the 'Id' column (not useful for prediction)\n",
    "2. Encode the 'Species' column to numbers using `.map()`:\n",
    "   - Iris-setosa â†’ 0\n",
    "   - Iris-versicolor â†’ 1\n",
    "   - Iris-virginica â†’ 2\n",
    "3. Create a new feature called `petal_area` (PetalLengthCm Ã— PetalWidthCm)\n",
    "4. Create a new feature called `sepal_area` (SepalLengthCm Ã— SepalWidthCm)\n",
    "5. Select only the numerical features for ML (exclude original Species column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4835328",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clean = iris_df.copy()\n",
    "\n",
    "# TODO: 1. Drop the 'Id' column\n",
    "\n",
    "# TODO: 2. Encode Species to numbers (hint: add the species_mapping dictionary to the map function)\n",
    "species_mapping = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}\n",
    "\n",
    "# TODO: 3 & 4. Create new features (a sneak peek into feature engineering!)]\n",
    "\n",
    "# TODO: 5. Select features for ML\n",
    "feature_cols = [] # Add the feature column names here\n",
    "X_iris = None # Replace None with the correct code to create the feature matrix\n",
    "y_iris = None # Replace None with the correct code to create the target vector\n",
    "\n",
    "print(\"Feature matrix shape:\", X_iris.shape)\n",
    "print(\"Target vector shape:\", y_iris.shape)\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "print(X_iris.head())\n",
    "print(\"\\nTarget values:\")\n",
    "print(y_iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec04cf",
   "metadata": {},
   "source": [
    "### Bonus Exercise 3: Visualize with a Scatter Plot\n",
    "\n",
    "**Task:**\n",
    "Create a scatter plot showing the relationship between petal length and petal width, with points colored by species.\n",
    "\n",
    "This will help you see how the three species cluster differently based on their petal measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb95298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "colors = {'Iris-setosa': 'red', 'Iris-versicolor': 'blue', 'Iris-virginica': 'green'}\n",
    "\n",
    "# Plot each species separately so we can color them differently\n",
    "for species in iris_clean['Species'].unique():\n",
    "    species_data = iris_clean[iris_clean['Species'] == species]\n",
    "    \n",
    "    # TODO: Replace None with the columns to create the scatter plot of the petal length vs petal width\n",
    "    plt.scatter(None, \n",
    "                None,\n",
    "                c=colors[species],\n",
    "                label=species,\n",
    "                alpha=0.7,  # Slight transparency\n",
    "                s=100,      # Size of points\n",
    "                edgecolors='black',\n",
    "                linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('Iris Species: Petal Length vs Petal Width', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Species', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŒ¸ Visualization Insights:\")\n",
    "print(\"- Iris-setosa (red) is clearly separated with smaller petals\")\n",
    "print(\"- Iris-versicolor (blue) and Iris-virginica (green) have some overlap\")\n",
    "print(\"- Petal measurements are excellent features for classification!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
